{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5_Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Installing YOLOv5 Dependencies"
      ],
      "metadata": {
        "id": "qwLs0tHffjFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UBavUxNwMlVU",
        "outputId": "d06bf6fd-7eb6-445b-b2c2-fa6080764bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Setup complete. Using torch 1.11.0+cu113 (CPU)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5 &> /dev/null\n",
        "%pip install -qr requirements.txt &> /dev/null\n",
        "%pip install pyyaml==6.0\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detectron2 streamlit\n",
        "Thanks code from [Object Detection App with Streamlit and Detectron2 - Medium](https://medium.com/@bolarinwaoreoluwa24/object-detection-app-with-streamlit-and-detectron2-26776802cc00)"
      ],
      "metadata": {
        "id": "XCrDDK80foHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/Obstacle-Detection-for-Blind-people-Deployment')"
      ],
      "metadata": {
        "id": "6YGZBf8JytZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/xugaoxiang/yolov5-streamlit.git\""
      ],
      "metadata": {
        "id": "8Ooaxb-1Pl7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548cb4b4-4423-4627-b6fa-d5b6b063175b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5-streamlit'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Total 126 (delta 0), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (126/126), 31.61 MiB | 32.47 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/Shareddrives/AI Builders/Model Checkpoints/Dataset3.0/model_final.pth\" \"/content/Obstacle-Detection-for-Blind-people-Deployment/model\""
      ],
      "metadata": {
        "id": "5ss8mbp0wtlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up ngrok\n",
        "Thanks tutorial from [Create a runtime dashboard from Colab via Streamlit & Pyngrok](https://towardsdev.com/create-a-runtime-dashboard-from-colab-via-streamlit-pyngrok-3c5588cfcfc5)"
      ],
      "metadata": {
        "id": "nb5FpnByc4Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok \n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "UCq7YMJ6SHsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive For API Key"
      ],
      "metadata": {
        "id": "qBIvzL-jgDh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dfUFS41WgHKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c933b56-ef6b-43bb-9587-8cada5e8ced9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/API Keys/ngrok.txt') as f:\n",
        "    apikey = f.read()"
      ],
      "metadata": {
        "id": "UvZAbdaPrsnr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken $apikey"
      ],
      "metadata": {
        "id": "Y9oa1jbhc7ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499427bc-7090-4b87-8992-799d600e6e66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "publ_url = ngrok.connect()\n",
        "publ_url "
      ],
      "metadata": {
        "id": "1LKr3Ajvd9IT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4243d269-97f5-41fe-919e-87be9ccb8d71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://e7de-34-80-191-64.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start Streamlit!"
      ],
      "metadata": {
        "id": "b__EabxViwuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run --server.port 80 /content/yolov5/app.py"
      ],
      "metadata": {
        "id": "PoB4fW1jc_De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f367789-f00b-4f74-b0d4-a7860e628ffa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-20 19:35:42.165 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:80\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.80.191.64:80\u001b[0m\n",
            "\u001b[0m\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "2022-06-20 19:38:51.534 YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "2022-06-20 19:38:52.366 Fusing layers... \n",
            "Fusing layers... \n",
            "2022-06-20 19:38:55.189 Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "2022-06-20 19:38:55.195 Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "tensor([[3.76027e+02, 3.01806e+02, 4.51718e+02, 3.32165e+02, 8.99752e-01, 4.00000e+00],\n",
            "        [1.60456e+02, 3.31808e+02, 5.24217e+02, 4.60000e+02, 5.73403e-01, 5.00000e+00],\n",
            "        [1.77806e+02, 3.13248e+02, 2.73003e+02, 3.78306e+02, 2.84666e-01, 4.00000e+00]])\n",
            "2022-06-20 19:38:57.989 Saved 1 image to \u001b[1mruns/detect/exp2\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp2\u001b[0m\n",
            "2022-06-20 19:38:57.990 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 554, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 110, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 66, in main\n",
            "    imageInput(deviceoption)\n",
            "  File \"/content/yolov5/app.py\", line 25, in imageInput\n",
            "    img_ = Image.open(\"result.jpg\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2843, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'result.jpg'\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "2022-06-20 19:40:11.604 YOLOv5 ðŸš€ 2022-6-20 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ 2022-6-20 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ 2022-6-20 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ 2022-6-20 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "2022-06-20 19:40:11.921 Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "2022-06-20 19:40:14.595 Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "2022-06-20 19:40:14.601 Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "tensor([[0.00000e+00, 3.52992e+02, 3.91832e+02, 4.58874e+02, 3.25932e-01, 5.00000e+00],\n",
            "        [0.00000e+00, 4.18432e+02, 1.65776e+02, 4.59962e+02, 3.23212e-01, 1.00000e+00]])\n",
            "2022-06-20 19:40:17.432 Saved 1 image to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "2022-06-20 19:40:17.432 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 554, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 110, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 66, in main\n",
            "    imageInput(deviceoption)\n",
            "  File \"/content/yolov5/app.py\", line 25, in imageInput\n",
            "    img_ = Image.open(\"result.jpg\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2843, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'result.jpg'\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "2022-06-20 19:46:26.954 YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "2022-06-20 19:46:27.347 Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "2022-06-20 19:46:30.048 Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "2022-06-20 19:46:30.054 Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "tensor([[3.76027e+02, 3.01806e+02, 4.51718e+02, 3.32165e+02, 8.99752e-01, 4.00000e+00],\n",
            "        [1.60456e+02, 3.31808e+02, 5.24217e+02, 4.60000e+02, 5.73403e-01, 5.00000e+00],\n",
            "        [1.77806e+02, 3.13248e+02, 2.73003e+02, 3.78306e+02, 2.84666e-01, 4.00000e+00]])\n",
            "2022-06-20 19:46:32.895 Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "2022-06-20 19:46:32.899 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2847, in open\n",
            "    fp.seek(0)\n",
            "AttributeError: 'NoneType' object has no attribute 'seek'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 554, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 112, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 68, in main\n",
            "    imageInput(deviceoption)\n",
            "  File \"/content/yolov5/app.py\", line 23, in imageInput\n",
            "    predImg = Image.open(pred.show())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2849, in open\n",
            "    fp = io.BytesIO(fp.read())\n",
            "AttributeError: 'NoneType' object has no attribute 'read'\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "2022-06-20 19:51:03.981 YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "YOLOv5 ðŸš€ v6.1-258-g1156a32 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "2022-06-20 19:51:04.370 Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "Fusing layers... \n",
            "2022-06-20 19:51:07.207 Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "Model summary: 444 layers, 86233975 parameters, 0 gradients\n",
            "2022-06-20 19:51:07.213 Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "Adding AutoShape... \n",
            "tensor([[234.35461,   1.07311, 303.41345, 209.46671,   0.85326,   1.00000],\n",
            "        [431.71881, 397.14014, 562.27362, 450.56430,   0.78746,   1.00000],\n",
            "        [ 15.25580, 298.34232, 401.09290, 460.00000,   0.69399,   5.00000]])\n",
            "2022-06-20 19:51:10.027 Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "2022-06-20 19:51:10.030 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2847, in open\n",
            "    fp.seek(0)\n",
            "AttributeError: 'NoneType' object has no attribute 'seek'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 554, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 123, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 79, in main\n",
            "    imageInput(deviceoption)\n",
            "  File \"/content/yolov5/app.py\", line 25, in imageInput\n",
            "    predImg = Image.open(pred.show())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2849, in open\n",
            "    fp = io.BytesIO(fp.read())\n",
            "AttributeError: 'NoneType' object has no attribute 'read'\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/Shareddrives/AI Builders/final/yolov5xDataset3AWS.zip\" -d \"/content/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4QEJx_lZebk",
        "outputId": "26d5f7d3-b862-45b9-8f97-75ff03aeaeae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/Shareddrives/AI Builders/final/yolov5xDataset3AWS.zip\n",
            "   creating: /content/models/test/\n",
            "   creating: /content/models/test/.ipynb_checkpoints/\n",
            "  inflating: /content/models/test/.ipynb_checkpoints/confusion_matrix-checkpoint.png  \n",
            "  inflating: /content/models/test/confusion_matrix.png  \n",
            "  inflating: /content/models/test/F1_curve.png  \n",
            "  inflating: /content/models/test/PR_curve.png  \n",
            "  inflating: /content/models/test/P_curve.png  \n",
            "  inflating: /content/models/test/R_curve.png  \n",
            "  inflating: /content/models/test/val_batch0_labels.jpg  \n",
            "  inflating: /content/models/test/val_batch0_pred.jpg  \n",
            "  inflating: /content/models/test/val_batch1_labels.jpg  \n",
            "  inflating: /content/models/test/val_batch1_pred.jpg  \n",
            "  inflating: /content/models/test/val_batch2_labels.jpg  \n",
            "  inflating: /content/models/test/val_batch2_pred.jpg  \n",
            "   creating: /content/models/train/\n",
            "  inflating: /content/models/train/confusion_matrix.png  \n",
            "  inflating: /content/models/train/events.out.tfevents.1655602136.ip-172-31-18-65.8726.0  \n",
            "  inflating: /content/models/train/F1_curve.png  \n",
            "  inflating: /content/models/train/hyp.yaml  \n",
            "  inflating: /content/models/train/labels.jpg  \n",
            "  inflating: /content/models/train/labels_correlogram.jpg  \n",
            "  inflating: /content/models/train/opt.yaml  \n",
            "  inflating: /content/models/train/PR_curve.png  \n",
            "  inflating: /content/models/train/P_curve.png  \n",
            "  inflating: /content/models/train/results.csv  \n",
            "  inflating: /content/models/train/results.png  \n",
            "  inflating: /content/models/train/R_curve.png  \n",
            "  inflating: /content/models/train/train_batch0.jpg  \n",
            "  inflating: /content/models/train/train_batch1.jpg  \n",
            "  inflating: /content/models/train/train_batch2.jpg  \n",
            "  inflating: /content/models/train/val_batch0_labels.jpg  \n",
            "  inflating: /content/models/train/val_batch0_pred.jpg  \n",
            "  inflating: /content/models/train/val_batch1_labels.jpg  \n",
            "  inflating: /content/models/train/val_batch1_pred.jpg  \n",
            "  inflating: /content/models/train/val_batch2_labels.jpg  \n",
            "  inflating: /content/models/train/val_batch2_pred.jpg  \n",
            "   creating: /content/models/train/weights/\n",
            "  inflating: /content/models/train/weights/best.pt  \n",
            "  inflating: /content/models/train/weights/last.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/models/train/weights/best.pt\" \"/content/yolov5/weights/best.pt\""
      ],
      "metadata": {
        "id": "bIpSzJDXdixT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: \n",
        "\n",
        "*   Confidence Slider\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "YB8sng6rOgK9"
      }
    }
  ]
}